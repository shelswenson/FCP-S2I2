\subsection{Technologies} [\textit{Need input from group}]
\subsubsection{Initial Target Platforms}

\subsubsection{Emerging and Future Platforms}

\begin{itemize}
\item categories: new processors, fpgas, custom ASICs
\item need to maintain flexibility
  \begin{itemize}
  \item adapt quickly, but also ignore quickly if no pay off
  \item find how to protect end-users from vendor-specific access methods but allow access, nasty trade-off
  \item must find balance of problem specific v. general
    \begin{itemize}
    \item too problem-specific to support: bitcoin hash engines
    \item not too-problem-specific: FPGA sold to compute bitcoin hashes
    \end{itemize}
  \end{itemize}
\item keep a collection of ``benchmark'' problems (akin to the dwarves, but actual code)
  \begin{itemize}
  \item evaluate how each can map to new platforms
  \item provide initial implementations, crucial for developers starting out
  \item find abstractions across similar future platforms
  \item multiple levels of ``benchmarks''
    \begin{itemize}
    \item kernels: quickest to port, often will highlight precise architectural features, may not reflect application performance
    \item mini-applications: considerably more effort, involve both general and specific features, relects some aspects of application performance
    \item full applications: too much effort for the center, will partner with other S2I2s to assist in porting their applications if other benchmark levels indicate some advantage
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Resources}
An NSF center focused on accelerator research will need both hardware and software resources. While the need for hardware resources is obvious, the software resource component for accelerators is larger
than typically used for software application development. Each accelerator, be it a multi-core processor, GPU, or FPGA, needs a special purpose compiler to create the final bit-stream that is loaded on to the device. These compilers are typically supplied by the accelerator manufacturer at a reasonable price. The more expensive development tools used in accelerator development are the specialized tools for finding and exploiting parallelism, pipelining, threading, and/or vectorizing opportunities within the application. These tools can be relatively expensive and quite valuable for programmer productivity and optimal code design.

Starting with multi-core processors there is a long list of compilers that can be used to make binaries for those devices. Currently there are over five different C/C++/Fortran compilers that could be used: PGI, PathScale, Intel, GNU, and vendor supplied compilers like Cray's CCE compiler. In addition, there are companies working on next generation compiler languages and their compilers, like IBM's X10 and Cray's Chapel. Each of these compilers will have their own strengths and weaknesses, and the accelerator researcher will need to find which compiler works best for their application.

Coupled with the multi-core compilers are the libraries needed for optimal math operations like those in scientific libraries. These math libraries can be free, like the Goto library or cost money, like the NAG library. The developer will also need libraries to move data between the cores/accelerators, either PGAS mechanisms like UPC, Co-Array Fortran, Global arrays, or explicit data passing mechanisms like MPI, SHMEM, and Pthreads. Finally there are a host of tools used to debug non-working codes like Totalview, DDT, gdb, and tools to measure application performance and improve optimizations.

GPU devices are coupled with a general purpose processor, so all the development tools previously mentioned are also needed for GPU accelerated code development. An application using GPUs will need to be partitioned into separate CPU and GPU parts, and the GPU portion compiled with a GPU specific compiler. That compiler will come from the GPU manufacturer. The CPU portion is then compiled normally and the resulting bit-streams are joined into one executable.

Since FPGAs are used for making special purpose devices that can be used in consumer products, there are many expensive tools for making optimal designs. Tools like ModelSim, Symplicity, Simulink, MentorGraphs, Impulse-C, and many others, are used for laying out a circuit in the FPGA for optimal performance. These tools tend to be an order of magnitude more expensive than software compilers for multi-core and GPU devices. The FPGA will also need the manufacturer’s compiler to compile the final bit-stream that is loaded on the device. More advanced tools will also simulate the CPU and the FPGA code running together for more opportunities to debug the logic.

Finally, in addition to the large suite of software development tools, the researchers will need the accelerator devices themselves. A minimal requirement is to have a single multi-socketed system with multi-core, GPUs and FPGAs attached to the research center. Better yet, one accelerated system will be provided to each researcher. In an ideal scenario, one large multi-node system will be available with each node having multi-cores, GPUs and FPGAs. This way, each researcher will have their own node for accelerator research, and can also experiment with running an application across multiple accelerated nodes. There will also be cases where the accelerator may not be attached to a CPU, but to a separate
special purpose system, like Cray’s multi-threaded XMT. In this case a separate system will be needed for multi-threaded research.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "FPC_FinalReportMain"
%%% End: 
