\section{Executive Summary}
 This is a report on the activities and outcomes of the 
\textit{Accelerating Grand Challenge Data-Intensive 
Problems using Future Computing Platforms Project}
supported by the National Science Foundation (NSF) 
as part of the call for Scientific Software Innovation Institutes (S2I2) 
Conceptualization Proposals [NSF 11-589]. 

Solving scientific grand challenges requires effective use of cyberinfrastructure (CI).
Recent advances in computing technology can dramatically accelerate innovation 
to solve complex problems of societal importance. 
These \textit{Future Computing Platforms} include 
multi-core and many-core processors,
general-purpose graphics processing units (GPGPU),
field programmable gate arrays (FPGA), and
cloud computing. 
Future platforms provide distinctive features
of energy efficiency and performance beneficial 
to under-served scientific domains. 
Software forms the gateway for scientific research communities to access CI, 
and scientists need a critical mass of
sustainable software to benefit from future platforms' transformative potential.


We aim to organize scientific communities to leverage the disruptive potential of future
computing platforms while managing the software sustainability challenges that hamper solving
grand challenge problems.
In workshops, panels, and face-to-face meetings we engaged
domain scientists from biology and biomedicine, 
%%NEED TO ADD CYBER-INFRASTRUCTURE
and 
%%%DO WE ADD SECURITY AND SOCIAL SCIENCES?
computer science researchers working with various future platforms.
The goal of these engagements 
was to produce an in-depth conceptual design for a center that will enable scientists to leverage the disruptive potential of future computing platforms while managing the software sustainability challenges that hamper solving data-intensive grand challenge problems.


This executive summary presents our vision for a Center for Sustainable Software on Future Computing Platforms as well as key recommendations for its organization and goals;
 subsequent sections of the report provide  summarize the activities of the conceptualization project, and a more detailed stragegic plan for developing such a center.

\subsection{Vision}

Future computing platforms (FCP) must be a significant element of the S2I2 program if it is to have a broad impact on scientific software. 
While traditional CI makes available computing power, data storage, and network bandwidth, recent advances in computing technology have the ability to dramatically accelerate innovation to solve complex problems of societal importance. 
FCP provide distinctive features of energy efficiency, performance, and accessibility. 
These potentially disruptive platforms assist not only traditional high-performance computing (HPC)-based science but also data-intensive science areas such as biological sicence, 
social science, and security.  
These sciences need a \textit{Center for Sustainable Software on Future Computing Platforms} to identify, evaluae, and support future platforms relevant to scientists in data-intensive areas.

High-end computing systems are moving to extreme scales by augmenting architectural designs with hardware-based application accelerators. 
Examples of these accelerators are reconfigurable hardware such as FPGAs, heterogeneous processors like the upcoming Intel Knights family and AMD Fusion platforms, 
highly multi-core and multi-threaded processors from Cray and Oracle/Sun, and general-purpose graphics processing units (GPGPUs) from NVIDIA and AMD, among others. 
Current software repositories and tools do not address development for these systems sufficiently for productive scientific use because the application space is diverse, 
leading to an immense performance optimization design space.

Data-intensive computational kernels in biology, network security, and social sciences exemplify emerging and future workloads and represent challenging cases that can benefit from FCP. 
For example, next generation DNA sequencing instruments flood individual investigators with terabytes and community projects with petabytes of data. 
High-throughput instrumentation is also making a presence in other areas, such as materials science where atomic-scale images are provided by Atom Problem Tomography (APT). 
A combination of multi-core processors, and accelerator technologies such as GPUs and FPGAs will provide a more affordable and accessible alternative to large clusters and supercomputers. 
Another application for accelerators is the area of high throughput malware scanning, matching large data quickly against sophisticated malware signatures. 
Compute-intensive numerical methods, such as the molecular dynamics and Fast Multi-pole Methods, can also benefit from accelerator-driven parallelization.

Working through practical details of how applications can take advantage of such FCP will enable breakthroughs in these domains. 
An iterative development process is required to understand not only the features and capabilities, but also the limitations and idiosyncrasies of new technologies useful in solving science problems efficiently. 
There is a vital need and present opportunity to develop a community-based collaborative software infrastructure that aims to accelerate the design, implementation, optimization, and dissemination of a wide variety of codes tuned for emerging technologies, as well as methodologies, education and training materials.

\paragraph{Vision:} The \textit{Center for Sustainable Software on Future Computing Platforms} will provide scientific communities the means to leverage future platformsâ€™ potential to transform research and gain key insights into data-intensive grand challenge problems.


\subsection{Oganization, goals, and activities}
The S2I2 center's operational model must balance research, development, and integration activities to best achieve the overall goal of software creation, maintenance, and sustainability. Such a center requires significant coordination and effort, and splitting of the center into many small, distributed parts would be counter-productive.  
Thus, the center model should include a core group at a single site, and a limited number of partners at remote sites. 
The center is best organized through a strong management model headed by a center director with academic credentials and management experience. 
An S2I2 center should establish an external advisory board representing target communities, including industry, investigators from Scientific Software Elements (SSE) and Scientific Software Integration (SSI) projects [NSF 14-520], and notable scientists.

The S2I2 center management should identify and engage with relevant small-scale SSE's and medium- scale SSI's. Shared funding for aligned activities can provide an SSE/SSI incentive to collaborate. 
The center management should focus on a set of key projects and activities with high impact on the targeted communities, including software development, maintenance, user education, and training. 
Dedicated staff with software development expertise in FCP needs to provide in-depth support for domain scientists, up to several months at a time, to facilitate their leveraging new accelerator technologies autonomously.

The primary goal of the center must be to investigate, build, distribute, and maintain software that will spur innovation in scientific computing over an extended period of time. 
A strong software design and development team should build and maintain reliable, usable, and extensible software that are created organically and contributed by user, developer, and business communities.

A sustainable software infrastructure requires building the ``software right". 
We recommend that a center should adopt portable, open, extensible software development frameworks and practices to allow easy intake, sharing, and sustained maintenance of code. 
Software should be modular, with stakeholders associated with each module. Growth of new software must be encouraged while balancing the need to maintain the existing codebase. 
The center must actively plan for the lifecycle of software, transitioning from management to maintenance and even to deprecation. 
Maintaining and evaluating an evolving set of benchmarks and ``hero" codes is important to the community.
Any center-maintained software must include and enforce guidelines for code contribution.

Documentation of both software and best practices for specific domains and applications are essential. Availability of trained technical writers in the center would be of immediate value to the community.

Choosing the ``right software" to maintain in the center is important. 
The center should target science domains and applications that have impact in the near and long term (2-3 years). 
A balanced selection of high risk software applications that may advance sciences radically, as well as software with incremental, sustained improvements should be chosen. 
The center must provide avenues for computer science investigators to participate, to ensure rapid incorporation of advanced software tools.

Education and training of external visitors is a major aspect of attaining sustainability as it can spread expertise at their home institution and beyond. 
Invitations to long-term visitors could be handled by a bidding mechanism, any form of peer review, or optionally in return for donating their application code. 
The center needs good instructors, both for large groups and for one-to-one consulting basis.

The center's lifespan should go through phases: an initial start-up phase to collect code and best practices, followed by delivery of major artifacts to the wider community, and then a push of knowledge outward through education and training while still curating the wider community's knowledge. 
Any center must prove value not only to the NSF but also to community stakeholders like vendors and users. 
We recommend the NSF to encourage specific letters of collaboration from the broadest possible community of scientific software researchers, developers, and domain scientists in support of an S2I2 proposal.


